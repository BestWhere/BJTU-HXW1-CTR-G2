{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线上分数：0.81\n",
    "\n",
    "\n",
    "# ----------------环境配置----------------\n",
    "# 安装相关依赖库 如果是windows系统，cmd命令框中输入pip安装，参考上述环境配置\n",
    "# !pip install sklearn\n",
    "# !pip install pandas\n",
    "# !pip install catboost\n",
    "# --------------------------------------\n",
    "import catboost\n",
    "# ----------------导入库-----------------\n",
    "# 数据探索模块使用第三方库\n",
    "from pandas.core.frame import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "# 核心模型使用第三方库\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "# 交叉验证所使用的第三方库\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "# 评估指标所使用的的第三方库\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "# 忽略报警所使用的第三方库\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "print('import Ending...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 禁止hash随机化\n",
    "set_seed(2022)\n",
    "# --------------------------------------\n",
    "\n",
    "# ----------------数据预处理-------------\n",
    "# 读取训练数据和测试数据\n",
    "train_data_ads = pd.read_csv('../dataset/train.csv')\n",
    "train_data_feeds = pd.read_csv('../dataset/train_feeds.csv')\n",
    "\n",
    "test_data_ads = pd.read_csv('../dataset/final_test.csv')\n",
    "test_data_feeds = pd.read_csv('../dataset/test_feeds.csv')\n",
    "\n",
    "\n",
    "# 合并数据\n",
    "train_data_ads['istest'] = 0\n",
    "test_data_ads['istest'] = 1\n",
    "data_ads = pd.concat([train_data_ads, test_data_ads], axis=0, ignore_index=True)\n",
    "\n",
    "train_data_feeds['istest'] = 0\n",
    "test_data_feeds['istest'] = 1\n",
    "data_feeds = pd.concat([train_data_feeds, test_data_feeds], axis=0, ignore_index=True)\n",
    "\n",
    "del train_data_ads, test_data_ads, train_data_feeds, test_data_feeds\n",
    "gc.collect()\n",
    "print('预处理 Ending...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------特征工程---------------\n",
    "# 包含自然数编码、特征提取和内存压缩三部分内容。\n",
    "# 1、自然数编码\n",
    "def label_encode(series, series2):\n",
    "    unique = list(series.unique())\n",
    "    return series2.map(dict(zip(\n",
    "        unique, range(series.nunique())\n",
    "    )))\n",
    "\n",
    "\n",
    "for col in ['ad_click_list_v001', 'ad_click_list_v002', 'ad_click_list_v003', 'ad_close_list_v001',\n",
    "            'ad_close_list_v002', 'ad_close_list_v003', 'u_newsCatInterestsST']:\n",
    "    data_ads[col] = label_encode(data_ads[col], data_ads[col])\n",
    "print('自然编码 Ending...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2、特征提取\n",
    "# data_feeds特征构建\n",
    "# 特征提取部分围绕着data_feeds进行构建（添加源域信息）\n",
    "# 主要是nunique属性数统计和mean均值统计。\n",
    "# 由于是baseline方案，所以整体的提取比较粗暴，大家还是有很多的优化空间。\n",
    "\n",
    "\n",
    "# -------------------------------1. nunique属性数统计特征-------------------------------------------\n",
    "print('nunique属性数统计特征 Starting...')\n",
    "cols = [f for f in data_feeds.columns if f not in ['label', 'istest', 'u_userId']]\n",
    "for col in tqdm(cols):\n",
    "    tmp = data_feeds.groupby(['u_userId'])[col].nunique().reset_index()\n",
    "    tmp.columns = ['user_id', col + '_feeds_nuni']\n",
    "    data_ads = data_ads.merge(tmp, on='user_id', how='left')\n",
    "print('nunique属性数统计特征 Ending...')\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------2. mean均值统计特征------------------------------------------------\n",
    "print('mean均值统计特征 Starting...')\n",
    "cols = [f for f in data_feeds.columns if\n",
    "        f not in ['istest', 'u_userId', 'u_newsCatInterests', 'u_newsCatDislike', 'u_newsCatInterestsST',\n",
    "                  'u_click_ca2_news', 'i_docId', 'i_s_sourceId', 'i_entities']]\n",
    "for col in tqdm(cols):\n",
    "    tmp = data_feeds.groupby(['u_userId'])[col].mean().reset_index()\n",
    "    tmp.columns = ['user_id', col + '_feeds_mean']\n",
    "    data_ads = data_ads.merge(tmp, on='user_id', how='left')\n",
    "print('mean均值统计特征 Ending...')\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -------------------------------3. 穿越特征------------------------------------------------\n",
    "print('穿越特征 Starting...')\n",
    "data_ads['month'] = data_ads['pt_d'].apply(lambda x: int(str(x)[4:6]))\n",
    "data_ads['day'] = data_ads['pt_d'].apply(lambda x: int(str(x)[6:8]))\n",
    "# data_ads['hour'] = data_ads['pt_d'].apply(lambda x: int(str(x)[8:10]))\n",
    "# data_ads['minu'] = data_ads['pt_d'].apply(lambda x: int(str(x)[10:12]))\n",
    "data_ads['date'] = data_ads['month']*30 + data_ads['day']\n",
    "\n",
    "\n",
    "def get_date_feature(data, gap_list=[1], col=['user_id']):\n",
    "\n",
    "    for gap in gap_list:\n",
    "\n",
    "        # 后面时间-当前时间\n",
    "        data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] = data.groupby(col)['date'].shift(-gap)\n",
    "        data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] = data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] - \\\n",
    "                                                                data['date']\n",
    "\n",
    "        # 前面时间-当前时间\n",
    "        data['ts_{}_{}_diff_last'.format('_'.join(col), gap)] = data.groupby(col)['date'].shift(+gap)\n",
    "        data['ts_{}_{}_diff_last'.format('_'.join(col), gap)] = data['date'] - data[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)]\n",
    "\n",
    "        # 统计不为nan的值，做差前有曝光，做差后就不会为nan。\n",
    "        data['ts_{}_{}_diff_next_count'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('count')\n",
    "        data['ts_{}_{}_diff_last_count'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('count')\n",
    "\n",
    "        # 统计时间差的平均值\n",
    "        data['ts_{}_{}_diff_next_mean'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('mean')\n",
    "        data['ts_{}_{}_diff_last_mean'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('mean')\n",
    "\n",
    "        # 统计时间差的最大值\n",
    "        data['ts_{}_{}_diff_next_max'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('max')\n",
    "        data['ts_{}_{}_diff_last_max'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('max')\n",
    "\n",
    "        # 统计时间差的最小值\n",
    "        data['ts_{}_{}_diff_next_min'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('min')\n",
    "        data['ts_{}_{}_diff_last_min'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('min')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_diff_date(data, gap_list=[1, 2, 3], col=['user_id'], con_list=[1], f='next'):\n",
    "    for gap in gap_list:\n",
    "        for con in con_list:\n",
    "            data['ts_s_{}_{}_{}_next_{}'.format(f, '_'.join(col), gap, con)] = data.groupby(col)[\n",
    "                'ts_{}_{}_diff_{}'.format('_'.join(col), con, f)].shift(-gap)\n",
    "            data['ts_s_{}_{}_{}_next_{}'.format(f, '_'.join(col), gap, con)] = data['ts_s_{}_{}_{}_next_{}'.format(f,\n",
    "                                                                                                                   '_'.join(\n",
    "                                                                                                                       col),\n",
    "                                                                                                                   gap,\n",
    "                                                                                                                   con)] - \\\n",
    "                                                                               data['ts_{}_{}_diff_{}'.format(\n",
    "                                                                                   '_'.join(col), con, f)]\n",
    "\n",
    "            data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)] = data.groupby(col)[\n",
    "                'ts_{}_{}_diff_{}'.format('_'.join(col), con, f)].shift(+gap)\n",
    "            data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)] = data['ts_{}_{}_diff_{}'.format(\n",
    "                '_'.join(col), con, f)] - data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "for col in [\n",
    "    ['user_id'], ['task_id'], ['adv_id'],\n",
    "    ['user_id', 'adv_id'], ['user_id', 'task_id'], ['user_id', 'creat_type_cd'],\n",
    "    ['user_id', 'adv_prim_id'], ['user_id', 'inter_type_cd'], ['user_id', 'slot_id'],\n",
    "    ['user_id', 'site_id'], ['user_id', 'spread_app_id']\n",
    "]:\n",
    "    print('_'.join(col), 'make', 'feature')\n",
    "    data_ads = get_date_feature(data_ads, gap_list=[1, 2, 3], col=col)\n",
    "    data_ads = get_diff_date(data_ads, gap_list=[1, 2, 3], col=col, con_list=[1], f='next')\n",
    "    data_ads = get_diff_date(data_ads, gap_list=[1, 2, 3], col=col, con_list=[1], f='last')\n",
    "\n",
    "\n",
    "print('穿越特征 Ending...')\n",
    "# -------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3、内存压缩\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "                start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 压缩使用内存\n",
    "# 由于数据比较大，所以合理的压缩内存节省空间尤为的重要\n",
    "# 使用reduce_mem_usage函数可以压缩近70%的内存占有。\n",
    "data_ads = reduce_mem_usage(data_ads)\n",
    "# Mem. usage decreased to 2351.47 Mb (69.3% reduction)\n",
    "# --------------------------------------\n",
    "\n",
    "# ----------------数据集划分-------------\n",
    "# 划分训练集和测试集\n",
    "cols = [f for f in data_ads.columns if f not in ['label', 'istest']]\n",
    "x_train = data_ads[data_ads.istest == 0][cols]\n",
    "x_test = data_ads[data_ads.istest == 1][cols]\n",
    "\n",
    "y_train = data_ads[data_ads.istest == 0]['label']\n",
    "\n",
    "del data_ads, data_feeds\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "\n",
    "# ----------------训练模型---------------\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, seed=2022):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} {}************************************'.format(str(i + 1),\n",
    "                                                                                                      str(seed)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], \\\n",
    "                                     train_y[valid_index]\n",
    "# # gpu版本\n",
    "#         params = {'learning_rate': 0.3, \n",
    "#                   'depth': 5,\n",
    "#                     'l2_leaf_reg': 10,\n",
    "#                     'bootstrap_type': 'Bernoulli',\n",
    "#                   'random_seed': seed,\n",
    "#                   'od_type': 'Iter', \n",
    "#                   'od_wait': 50, \n",
    "#                   'allow_writing_files': False,\n",
    "#                   'task_type' : 'GPU'}\n",
    "\n",
    "\n",
    "# cpu版本\n",
    "        params = {'learning_rate': 0.3, \n",
    "                  'depth': 5,\n",
    "                    'l2_leaf_reg': 10,\n",
    "                    'bootstrap_type': 'Bernoulli',\n",
    "                  'random_seed': seed,\n",
    "                  'od_type': 'Iter', \n",
    "                  'od_wait': 50, \n",
    "                  'allow_writing_files': False,\n",
    "                  }\n",
    "        \n",
    "        model = clf(iterations=20000, **params, eval_metric='AUC')\n",
    "        model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                  metric_period=200,\n",
    "                  cat_features=[],\n",
    "                  use_best_model=True,\n",
    "                  verbose=1)\n",
    "\n",
    "        val_pred = model.predict_proba(val_x)[:, 1]\n",
    "        test_pred = model.predict_proba(test_x)[:, 1]\n",
    "\n",
    "        train[valid_index] = val_pred\n",
    "        test += test_pred / kf.n_splits\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "\n",
    "        print(cv_scores)\n",
    "\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "cat_train, cat_test = cv_model(CatBoostClassifier, x_train, y_train, x_test, \"cat\")\n",
    "# --------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------结果保存---------------\n",
    "test_pred = cat_test\n",
    "df = {'pctr': test_pred}\n",
    "df = DataFrame(df)\n",
    "df = df.reset_index()\n",
    "df.columns = ['id', 'pctr']\n",
    "df.to_csv('./submission.csv', index=False)\n",
    "\n",
    "# x_test['pctr'] = cat_test\n",
    "# x_test[['log_id', 'pctr']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
