{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------环境配置----------------\n",
    "# 安装相关依赖库 如果是windows系统，cmd命令框中输入pip安装，参考上述环境配置\n",
    "# !pip install sklearn\n",
    "# !pip install pandas\n",
    "# !pip install catboost\n",
    "# --------------------------------------\n",
    "import catboost\n",
    "# ----------------导入库-----------------\n",
    "# 数据探索模块使用第三方库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import *\n",
    "# 核心模型使用第三方库\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "# 交叉验证所使用的第三方库\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "# 评估指标所使用的的第三方库\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "# 忽略报警所使用的第三方库\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 禁止hash随机化\n",
    "set_seed(2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "\n",
    "# ----------------数据预处理-------------\n",
    "# 读取训练数据和测试数据\n",
    "train_data_ads = pd.read_csv('../dataset/train.csv')\n",
    "train_data_feeds = pd.read_csv('../dataset/train_feeds.csv')\n",
    "\n",
    "test_data_ads = pd.read_csv('../dataset/final_test.csv')\n",
    "test_data_feeds = pd.read_csv('../dataset/test_feeds.csv')\n",
    "\n",
    "# 合并数据\n",
    "train_data_ads['istest'] = 0\n",
    "test_data_ads['istest'] = 1\n",
    "data_ads = pd.concat([train_data_ads, test_data_ads], axis=0, ignore_index=True)\n",
    "\n",
    "train_data_feeds['istest'] = 0\n",
    "test_data_feeds['istest'] = 1\n",
    "data_feeds = pd.concat([train_data_feeds, test_data_feeds], axis=0, ignore_index=True)\n",
    "\n",
    "del train_data_ads, test_data_ads, train_data_feeds, test_data_feeds\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# ----------------特征工程---------------\n",
    "# 包含自然数编码、特征提取和内存压缩三部分内容。\n",
    "# 1、自然数编码\n",
    "def label_encode(series, series2):\n",
    "    unique = list(series.unique())\n",
    "    return series2.map(dict(zip(\n",
    "        unique, range(series.nunique())\n",
    "    )))\n",
    "\n",
    "\n",
    "for col in ['ad_click_list_v001', 'ad_click_list_v002', 'ad_click_list_v003', 'ad_close_list_v001',\n",
    "            'ad_close_list_v002', 'ad_close_list_v003', 'u_newsCatInterestsST']:\n",
    "    data_ads[col] = label_encode(data_ads[col], data_ads[col])\n",
    "\n",
    "# 2、特征提取\n",
    "# data_feeds特征构建\n",
    "# 特征提取部分围绕着data_feeds进行构建（添加源域信息）\n",
    "# 主要是nunique属性数统计和mean均值统计。\n",
    "# 由于是baseline方案，所以整体的提取比较粗暴，大家还是有很多的优化空间。\n",
    "\n",
    "\n",
    "# -------------------------------1. nunique属性数统计特征-------------------------------------------\n",
    "print('nunique属性数统计特征 Starting...')\n",
    "cols = [f for f in data_feeds.columns if f not in ['label', 'istest', 'u_userId']]\n",
    "for col in tqdm(cols):\n",
    "    tmp = data_feeds.groupby(['u_userId'])[col].nunique().reset_index()\n",
    "    tmp.columns = ['user_id', col + '_feeds_nuni']\n",
    "    data_ads = data_ads.merge(tmp, on='user_id', how='left')\n",
    "print('nunique属性数统计特征 Ending...')\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------2. mean均值统计特征------------------------------------------------\n",
    "print('mean均值统计特征 Starting...')\n",
    "cols = [f for f in data_feeds.columns if\n",
    "        f not in ['istest', 'u_userId', 'u_newsCatInterests', 'u_newsCatDislike', 'u_newsCatInterestsST',\n",
    "                  'u_click_ca2_news', 'i_docId', 'i_s_sourceId', 'i_entities']]\n",
    "for col in tqdm(cols):\n",
    "    tmp = data_feeds.groupby(['u_userId'])[col].mean().reset_index()\n",
    "    tmp.columns = ['user_id', col + '_feeds_mean']\n",
    "    data_ads = data_ads.merge(tmp, on='user_id', how='left')\n",
    "print('mean均值统计特征 Ending...')\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------3. 穿越特征------------------------------------------------\n",
    "print('穿越特征 Starting...')\n",
    "# data_ads['month'] = data_ads['pt_d'].apply(lambda x: int(str(x)[4:6]))\n",
    "# data_ads['day'] = data_ads['pt_d'].apply(lambda x: int(str(x)[6:8]))\n",
    "# data_ads['hour'] = data_ads['pt_d'].apply(lambda x: int(str(x)[8:10]))\n",
    "# data_ads['minu'] = data_ads['pt_d'].apply(lambda x: int(str(x)[10:12]))\n",
    "# data_ads['date'] = data_ads['day']*1440 + data_ads['hour']*60 + data_ads['minu']\n",
    "data_ads['month'] = data_ads['pt_d'].apply(lambda x: int(str(x)[4:6]))  # 左闭右开\n",
    "data_ads['day'] = data_ads['pt_d'].apply(lambda x: int(str(x)[6:8]))\n",
    "# data_ads['hour'] = data_ads['pt_d'].apply(lambda x: int(str(x)[8:10]))\n",
    "# data_ads['minu'] = data_ads['pt_d'].apply(lambda x: int(str(x)[10:12]))\n",
    "# data_ads['date'] = data_ads['day']*1440 + data_ads['hour']*60 + data_ads['minu']\n",
    "data_ads['date'] = data_ads['month']*30+data_ads['day']\n",
    "\n",
    "\n",
    "def get_date_feature(data, gap_list=[1], col=['user_id']):\n",
    "\n",
    "    for gap in gap_list:\n",
    "\n",
    "        # 后面时间-当前时间\n",
    "        data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] = data.groupby(col)['date'].shift(-gap)\n",
    "        data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] = data['ts_{}_{}_diff_next'.format('_'.join(col), gap)] - \\\n",
    "                                                                data['date']\n",
    "\n",
    "        # 前面时间-当前时间\n",
    "        data['ts_{}_{}_diff_last'.format('_'.join(col), gap)] = data.groupby(col)['date'].shift(+gap)\n",
    "        data['ts_{}_{}_diff_last'.format('_'.join(col), gap)] = data['date'] - data[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)]\n",
    "\n",
    "        # 统计不为nan的值，做差前有曝光，做差后就不会为nan。\n",
    "        data['ts_{}_{}_diff_next_count'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('count')\n",
    "        data['ts_{}_{}_diff_last_count'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('count')\n",
    "\n",
    "        # 统计时间差的平均值\n",
    "        data['ts_{}_{}_diff_next_mean'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('mean')\n",
    "        data['ts_{}_{}_diff_last_mean'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('mean')\n",
    "\n",
    "        # 统计时间差的最大值\n",
    "        data['ts_{}_{}_diff_next_max'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('max')\n",
    "        data['ts_{}_{}_diff_last_max'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('max')\n",
    "\n",
    "        # 统计时间差的最小值\n",
    "        data['ts_{}_{}_diff_next_min'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_next'.format('_'.join(col), gap)].transform('min')\n",
    "        data['ts_{}_{}_diff_last_min'.format('_'.join(col), gap)] = data.groupby(col)[\n",
    "            'ts_{}_{}_diff_last'.format('_'.join(col), gap)].transform('min')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_diff_date(data, gap_list=[1, 2, 3], col=['user_id'], con_list=[1], f='next'):\n",
    "    for gap in gap_list:\n",
    "        for con in con_list:\n",
    "            data['ts_s_{}_{}_{}_next_{}'.format(f, '_'.join(col), gap, con)] = data.groupby(col)[\n",
    "                'ts_{}_{}_diff_{}'.format('_'.join(col), con, f)].shift(-gap)\n",
    "            data['ts_s_{}_{}_{}_next_{}'.format(f, '_'.join(col), gap, con)] = data['ts_s_{}_{}_{}_next_{}'.format(f,\n",
    "                                                                                                                   '_'.join(\n",
    "                                                                                                                       col),\n",
    "                                                                                                                   gap,\n",
    "                                                                                                                   con)] - \\\n",
    "                                                                               data['ts_{}_{}_diff_{}'.format(\n",
    "                                                                                   '_'.join(col), con, f)]\n",
    "\n",
    "            data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)] = data.groupby(col)[\n",
    "                'ts_{}_{}_diff_{}'.format('_'.join(col), con, f)].shift(+gap)\n",
    "            data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)] = data['ts_{}_{}_diff_{}'.format(\n",
    "                '_'.join(col), con, f)] - data['ts_s_{}_{}_{}_last_{}'.format(f, '_'.join(col), gap, con)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "for col in [\n",
    "    ['user_id'], ['task_id'], ['adv_id'],\n",
    "    ['user_id', 'adv_id'], ['user_id', 'task_id'], ['user_id', 'creat_type_cd'],\n",
    "    ['user_id', 'adv_prim_id'], ['user_id', 'inter_type_cd'], ['user_id', 'slot_id'],\n",
    "    ['user_id', 'site_id'], ['user_id', 'spread_app_id']\n",
    "]:\n",
    "    print('_'.join(col), 'make', 'feature')\n",
    "    data_ads = get_date_feature(data_ads, gap_list=[1, 2, 3], col=col)\n",
    "    data_ads = get_diff_date(data_ads, gap_list=[1, 2, 3], col=col, con_list=[1], f='next')\n",
    "    data_ads = get_diff_date(data_ads, gap_list=[1, 2, 3], col=col, con_list=[1], f='last')\n",
    "\n",
    "\n",
    "print('穿越特征 Ending...')\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3、内存压缩\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "                start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# 压缩使用内存\n",
    "# 由于数据比较大，所以合理的压缩内存节省空间尤为的重要\n",
    "# 使用reduce_mem_usage函数可以压缩近70%的内存占有。\n",
    "data_ads = reduce_mem_usage(data_ads)\n",
    "# Mem. usage decreased to 2351.47 Mb (69.3% reduction)\n",
    "# --------------------------------------\n",
    "\n",
    "# ----------------数据集划分-------------\n",
    "# 划分训练集和测试集\n",
    "cols = [f for f in data_ads.columns if f not in ['label', 'istest']]\n",
    "x_train = data_ads[data_ads.istest == 0][cols]\n",
    "x_test = data_ads[data_ads.istest == 1][cols]\n",
    "\n",
    "y_train = data_ads[data_ads.istest == 0]['label']\n",
    "\n",
    "del data_ads, data_feeds\n",
    "gc.collect()\n",
    "\n",
    "# cpu版本\n",
    "cat_params = {'learning_rate': 0.3, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'random_seed': 2022, 'iterations': 20000, 'eval_metric':'AUC',\n",
    "            'od_type': 'Iter', 'od_wait': 50, 'allow_writing_files': False}\n",
    "\n",
    "lgb_params = { 'random_seed': 2022}\n",
    "xgb_params = {'random_seed': 2022}\n",
    "\n",
    "\n",
    "# gpu版本\n",
    "cat_params = {'learning_rate': 0.3, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'random_seed': 2022, 'iterations': 20000, 'eval_metric':'AUC',\n",
    "            'od_type': 'Iter', 'od_wait': 50, 'allow_writing_files': False,'task_type':'GPU'}\n",
    "lgb_params = { 'random_seed': 2022,'device':'gpu'}\n",
    "xgb_params = {'random_seed': 2022,'tree_method': 'gpu_hist'}\n",
    "\n",
    "clfs = {'cat':CatBoostClassifier(**cat_params), 'lgb':LGBMClassifier(**lgb_params), 'xgb':XGBClassifier(**xgb_params)}\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "\n",
    "\n",
    "\n",
    "print('*'*20)\n",
    "print('kf.n_splits*len(clfs): ', kf.n_splits*len(clfs))\n",
    "print('*'*20)\n",
    "\n",
    "\n",
    "\n",
    "def get_oof(clf_name):\n",
    "    print('*' * 10, '开始{}模型的五折交叉训练'.format(clf_name), '*' * 10)\n",
    "    model = clfs[clf_name]\n",
    "\n",
    "    oof_train = np.zeros((x_train.shape[0], ))\n",
    "    oof_test = np.zeros((x_test.shape[0], ))\n",
    "    oof_test_skf = np.empty((5, x_test.shape[0]))\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(x_train, y_train)):\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train[val_idx]\n",
    "\n",
    "\n",
    "        if clf_name == 'cat':\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      metric_period=200,\n",
    "                      cat_features=[],\n",
    "                      use_best_model=True,\n",
    "                      verbose=1)\n",
    "\n",
    "        elif clf_name == 'lgb':\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      verbose=1, eval_metric='AUC')\n",
    "\n",
    "        elif clf_name == 'xgb':\n",
    "            model.fit(trn_x, trn_y, eval_set=[(val_x, val_y)],\n",
    "                      verbose=1, eval_metric='auc')\n",
    "        else:\n",
    "            print('没模型了')\n",
    "\n",
    "\n",
    "        oof_train[val_idx] = model.predict_proba(val_x)[:, 1]\n",
    "        oof_test_skf[i, :] = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "def stack_model(oof_list, pred_list, y):\n",
    "    print('开始stacking....')\n",
    "    train_stack = np.hstack(oof_list)\n",
    "    test_stack = np.hstack(pred_list)\n",
    "\n",
    "    oof = np.zeros((train_stack.shape[0], ))\n",
    "    predictions = np.zeros((test_stack.shape[0], ))\n",
    "    scores = []\n",
    "\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(train_stack, y)):\n",
    "        print('*'*10,'训练到第{}折'.format(fold_),'*'*10)\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        # lr_params = {'random_seed': 2022}\n",
    "        clf = LogisticRegression() # **lr_params\n",
    "        clf.fit(trn_data, trn_y)\n",
    "\n",
    "        oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n",
    "        predictions += clf.predict_proba(test_stack)[:, 1] / 5\n",
    "\n",
    "        score_single = roc_auc_score(val_y, oof[val_idx])\n",
    "        scores.append(score_single)\n",
    "        print(\"AUC:\", score_single)\n",
    "\n",
    "\n",
    "    print(\"score_list:\", scores)\n",
    "    print(\"score_mean:\", np.mean(scores))\n",
    "    print(\"score_std:\", np.std(scores))\n",
    "\n",
    "\n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "\n",
    "lgb_oof_train, lgb_oof_test = get_oof('lgb')\n",
    "xgb_oof_train, xgb_oof_test = get_oof('xgb')\n",
    "cat_oof_train, cat_oof_test = get_oof('cat')\n",
    "\n",
    "oof_list = [lgb_oof_train, xgb_oof_train, cat_oof_train]\n",
    "pred_list = [lgb_oof_test, xgb_oof_test, cat_oof_test]\n",
    "\n",
    "oof_stack, predictions_stack = stack_model(oof_list=oof_list, pred_list=pred_list, y=y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------结果保存---------------\n",
    "\n",
    "# clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5,\n",
    "#                                  max_depth=6, n_estimators=30)\n",
    "#\n",
    "# clf.fit(dataset_stacking_train, y_train)\n",
    "#\n",
    "# test = clf.predict_proba(dataset_stacking_test)[:, 1]\n",
    "\n",
    "# x_test['pctr'] = predictions_stack\n",
    "# x_test[['log_id', 'pctr']].to_csv('submission_model_fusion_stacking.csv', index=False)\n",
    "\n",
    "test_pred = predictions_stack\n",
    "# x_test['pctr'] = cat_test\n",
    "# x_test[['log_id', 'pctr']].to_csv('submission_xgb.csv', index=False)\n",
    "df = {'pctr': test_pred}\n",
    "df = DataFrame(df)\n",
    "df = df.reset_index()\n",
    "df.columns = ['id', 'pctr']\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
